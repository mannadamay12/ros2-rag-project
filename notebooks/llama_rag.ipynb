{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a94e477-0fe1-45c2-91ee-75ee5557bdbd",
   "metadata": {},
   "source": [
    "### Notes: If ImportError occurs, it's probably due to the huggingface-hub. \n",
    "> pip install huggingface-hub==0.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11faa2-3cbe-4b7f-93c3-29397ed9bec9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbac4516-42ca-4dfc-9fc2-01595f351a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"CPU\"\n",
    "HUGGING_FACE_TOKEN = os.environ.get('HUGGING_FACE_TOKEN') #in terminal: export HUGGING_FACE_TOKEN=\"YOUR_TOKEN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e9cf-f9eb-4fea-a6c7-44ec8126de3c",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f875935-7dcf-47f0-88e1-d68412712a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98d28a811f04ba49a569e1c7684a8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e153aa233184e40b20cfe390f520243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef5f3e90093462881cfc8b4b7ecad02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20e4bb6987241feaf4962ded9ece885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049c379d0f284118b4cd42e804a42c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bb0ffa06c24c82badf4e672e5bd392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306ff3188dc4461292171fe9fa68724e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18db81e5703848c7930f050108912734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968ecfe1fa8c479cac42f1be447636fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0c83b771fe438f8b3b8d781bdacc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Possible Models:\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- meta-llama/Llama-3.2-3B-Instruct\n",
    "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
    "'''\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\" \n",
    "\n",
    "# Quanitisize your model dtype (for sparsity)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Set token using ENV variable\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HUGGING_FACE_TOKEN)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea42f217-eb0c-4583-ade5-5832bbd9c7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongkyu/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/dongkyu/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's your name? I'm not sure if I should be calling you by a name or not. I'm not sure if I'm allowed to know your name.\n",
      "I'm not sure if I should be asking you this, but I'm curious. I'm a large\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"What's your name?\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "inputs = inputs.to('cpu')  # Ensure inputs are on CPU\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11b01be0-d0a9-418e-a0d4-3a172b30689a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"/home/dongkyu/RAG/knowledge\") #The Knowledge Base Folder\n",
    "docs = loader.load()\n",
    "len(docs) # this should give you number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e5b6c9-ce55-4694-9059-ebd546902055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154fafe3-7dd3-4303-a8de-38353a25998c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-base\",\n",
    "    model_kwargs={\"device\": DEVICE}\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e919ab43-cb51-4c54-8404-1d1bedd9c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Based on the information in this document provided in context, answer the question as accurately as possible in 1 or 2 lines. If the information is not in the context,\n",
    "respond with \"I don't know\" or a similar acknowledgment that the answer is not available.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. Do not provide commentary or elaboration more than 1 or 2 lines.\"\n",
    "\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8952142d-ec91-4c67-9aa0-b9a71910fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac8e41b5-cb60-4215-a297-400325190496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Here is a brief summary:\n",
      "\n",
      "Plasticity loss refers to the degradation of an agent's ability to learn and adapt to new information over time. This phenomenon was previously overlooked but has been gaining attention in recent years due to its implications for continual learning and deep reinforcement learning.\n",
      "\n",
      "In both domains, plasticity loss can be attributed to the gradual erosion of neural connections between neurons, leading to decreased performance on subsequent tasks. The causes of plasticity loss are multifaceted, including changes in task distributions, reduced novelty, and increased repetition.\n",
      "\n",
      "Researchers have proposed various methods to mitigate plasticity loss, such as regularization techniques, knowledge distillation, and transfer learning. However, these approaches often come with trade-offs, and there is still much to be learned about how to effectively address plasticity loss in different contexts.\n",
      "\n",
      "Overall, addressing plasticity loss remains a pressing challenge in both continual learning and deep reinforcement learning, requiring further research into its underlying mechanisms and effective mitigation strategies.\n"
     ]
    }
   ],
   "source": [
    "ask = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "result = ask(\"Give me a TLDR of this document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5840eda-a23d-4170-aafa-9f6c6732e014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Give me a TLDR of this document',\n",
       " 'result': \"[INST] <<SYS>>\\nUse the following pieces of context to answer the question at the end. Do not provide commentary or elaboration more than 1 or 2 lines.\\n<</SYS>>\\n\\n\\nthe continual learning literature has primarily focused on reducing catastrophic forgetting [Goodfellow et al., 2013,\\nKirkpatrick et al., 2017], more recently, the issue of plasticity loss has gained significant attention [Dohare et al.,\\n2021, 2023, Abbas et al., 2023]. Dohare et al. [2021] demonstrated that loss of plasticity sometimes becomes evident\\nonly after training for long sequences of tasks. Therefore, in continual learning, mitigating plasticity loss becomes\\nespecially important as agents encounter many tasks, or more generally a non-stationary data stream, over a long\\nlifetime.\\nReinforcement Learning. Plasticity loss has also gained significant attention in the deep reinforcement learning\\n(RL) literature [Igl et al., 2020, Kumar et al., 2020, Nikishin et al., 2022, Lyle et al., 2022, Gulcehre et al., 2022,\\nSokar et al., 2023, Nikishin et al., 2023, Lyle et al., 2023]. In RL, the input data stream exhibits two sources of\\n\\nthe continual learning literature has primarily focused on reducing catastrophic forgetting [Goodfellow et al., 2013,\\nKirkpatrick et al., 2017], more recently, the issue of plasticity loss has gained significant attention [Dohare et al.,\\n2021, 2023, Abbas et al., 2023]. Dohare et al. [2021] demonstrated that loss of plasticity sometimes becomes evident\\nonly after training for long sequences of tasks. Therefore, in continual learning, mitigating plasticity loss becomes\\nespecially important as agents encounter many tasks, or more generally a non-stationary data stream, over a long\\nlifetime.\\nReinforcement Learning. Plasticity loss has also gained significant attention in the deep reinforcement learning\\n(RL) literature [Igl et al., 2020, Kumar et al., 2020, Nikishin et al., 2022, Lyle et al., 2022, Gulcehre et al., 2022,\\nSokar et al., 2023, Nikishin et al., 2023, Lyle et al., 2023]. In RL, the input data stream exhibits two sources of\\n\\nQuestion: Give me a TLDR of this document\\n [/INST] \\n\\nHere is a brief summary:\\n\\nPlasticity loss refers to the degradation of an agent's ability to learn and adapt to new information over time. This phenomenon was previously overlooked but has been gaining attention in recent years due to its implications for continual learning and deep reinforcement learning.\\n\\nIn both domains, plasticity loss can be attributed to the gradual erosion of neural connections between neurons, leading to decreased performance on subsequent tasks. The causes of plasticity loss are multifaceted, including changes in task distributions, reduced novelty, and increased repetition.\\n\\nResearchers have proposed various methods to mitigate plasticity loss, such as regularization techniques, knowledge distillation, and transfer learning. However, these approaches often come with trade-offs, and there is still much to be learned about how to effectively address plasticity loss in different contexts.\\n\\nOverall, addressing plasticity loss remains a pressing challenge in both continual learning and deep reinforcement learning, requiring further research into its underlying mechanisms and effective mitigation strategies.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066187c-0111-4d9f-9d0d-5defa348cfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4654279-7e81-4a72-b1ce-c2dc8c501ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a8d90-ac6f-4e70-82c9-439df352c7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
