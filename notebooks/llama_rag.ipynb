{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a94e477-0fe1-45c2-91ee-75ee5557bdbd",
   "metadata": {},
   "source": [
    "### Notes: If ImportError occurs, it's probably due to the huggingface-hub. \n",
    "> pip install huggingface-hub==0.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11faa2-3cbe-4b7f-93c3-29397ed9bec9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbac4516-42ca-4dfc-9fc2-01595f351a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"CPU\"\n",
    "DEVICE = \"cpu\"\n",
    "HUGGING_FACE_TOKEN = os.environ.get('HUGGING_FACE_TOKEN') #in terminal: export HUGGING_FACE_TOKEN=\"YOUR_TOKEN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e9cf-f9eb-4fea-a6c7-44ec8126de3c",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f875935-7dcf-47f0-88e1-d68412712a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d602a0139b4035a622cc403230fa8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Possible Models:\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- meta-llama/Llama-3.2-3B-Instruct\n",
    "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
    "'''\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\" \n",
    "\n",
    "# Quanitisize your model dtype (for sparsity)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Set token using ENV variable\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HUGGING_FACE_TOKEN)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea42f217-eb0c-4583-ade5-5832bbd9c7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongkyu/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/dongkyu/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI! I'm so excited to share my latest project with you all! I've been working on a new line of eco-friendly, cruelty-free, and vegan-friendly makeup products, and I just can't wait to show you all the gorgeous shades and formulas I\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"HI!\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "inputs = inputs.to('cpu')  # Ensure inputs are on CPU\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bec46-9f0d-4994-8d74-c1624b82c354",
   "metadata": {},
   "source": [
    "### Knowledge Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde179ee-3690-4ec2-a867-c0aa68a42d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdf base\n",
    "pdf_base_path = '/home/dongkyu/RAG/knowledge'\n",
    "loader = PyPDFDirectoryLoader(pdf_base_path) #The Knowledge Base Folder\n",
    "pdf_docs = loader.load()\n",
    "len(pdf_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c9f652e-2267-48d6-8a03-fd19df4e4c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knowledge base\n",
    "#knowledge_base_path= \"/home/dongkyu/ros2-rag-project/notebooks/scraped_docs\"\n",
    "knowledge_base_path= \"/home/dongkyu/ros2-rag-project/exported_docs/ros2\"\n",
    "txt_loader = DirectoryLoader(knowledge_base_path, glob=\"**/*.txt\")\n",
    "docs = txt_loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e5b6c9-ce55-4694-9059-ebd546902055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154fafe3-7dd3-4303-a8de-38353a25998c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-base\",\n",
    "    model_kwargs={\"device\": DEVICE}\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dd4e7-d957-4780-9cd9-b6cbc1f79590",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e919ab43-cb51-4c54-8404-1d1bedd9c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Based on the information in this document provided in context, answer the question as accurately as possible in 1 or 2 lines. If the information is not in the context,\n",
    "respond with \"I don't know\" or a similar acknowledgment that the answer is not available.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. Do not provide commentary or elaboration more than 1 or 2 lines.?\"\n",
    "\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8952142d-ec91-4c67-9aa0-b9a71910fe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_592263/3144239959.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac8e41b5-cb60-4215-a297-400325190496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_592263/1986214230.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = ask(\"Write a sample code for Bouncy Bolson\")\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "```csharp\n",
      "using System;\n",
      "using System.Collections.Generic;\n",
      "\n",
      "public class Node {\n",
      "    public int Value { get; set; }\n",
      "    public List<Node> Next { get; set; }\n",
      "\n",
      "    public Node(int value) {\n",
      "        Value = value;\n",
      "        Next = new List<Node>();\n",
      "    }\n",
      "}\n",
      "\n",
      "public class BouncyBolson {\n",
      "    private Node head;\n",
      "\n",
      "    public void AddNode(int value) {\n",
      "        // implementation here\n",
      "    }\n",
      "\n",
      "    public void PrintList() {\n",
      "        // implementation here\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Here is the complete code with all functions implemented.\n",
      "\n",
      "```csharp\n",
      "using System;\n",
      "using System.Collections.Generic;\n",
      "\n",
      "public class Node {\n",
      "    public int Value { get; set; }\n",
      "    public List<Node> Next { get; set; }\n",
      "\n",
      "    public Node(int value) {\n",
      "        Value = value;\n",
      "        Next = new List<Node>();\n",
      "    }\n",
      "}\n",
      "\n",
      "public class BouncyBolson {\n",
      "    private Node head;\n",
      "\n",
      "    public BouncyBolson() {\n",
      "        head = null;\n",
      "    }\n",
      "\n",
      "    public void AddNode(int value) {\n",
      "        if (head == null)\n",
      "            head = new Node(value);\n",
      "        else {\n",
      "            Node current = head;\n",
      "            while (current.Next.Count > 0)\n",
      "                current = current.Next.Last();\n",
      "            current.Next.Add(new Node(value));\n",
      "        }\n",
      "    }\n",
      "\n",
      "    public void PrintList() {\n",
      "        Node current = head;\n",
      "        while (current!= null)\n",
      "            Console.WriteLine(current.Value + \" \");\n",
      "        Console.WriteLine();\n",
      "    }\n",
      "}\n",
      "```\n",
      " \n",
      "The `AddNode` function should add a node with given value to the rightmost position in the list, and the `PrintList` function should print out values of nodes from left to right.\n",
      " \n",
      "## Step-by-Step Solution\n",
      "\n",
      "To implement this problem, we need to create two main methods - one that adds a new node to the rightmost position of the linked list (`AddNode`) and another method that prints out the values of all nodes in the list (`PrintList`). Here's how you can do it:\n",
      "\n",
      "\n",
      "### Adding New Nodes\n",
      "\n",
      "\n",
      "We start by checking whether our list is empty. If it is, we simply assign the new node as the head of the list.\n",
      "\n",
      "\n",
      "If there are already elements on the list, we traverse through them until we reach the last element. We then append the new node to its next pointer.\n",
      "\n",
      "\n",
      "This ensures that each time we call `AddNode`, the newly added node will be appended to the right side of the existing list.\n",
      "\n",
      "\n",
      "### Printing\n"
     ]
    }
   ],
   "source": [
    "ask = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "result = ask(\"Write a sample code for Bouncy Bolson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066187c-0111-4d9f-9d0d-5defa348cfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4654279-7e81-4a72-b1ce-c2dc8c501ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a8d90-ac6f-4e70-82c9-439df352c7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
