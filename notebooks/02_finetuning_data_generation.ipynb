{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a94e477-0fe1-45c2-91ee-75ee5557bdbd",
   "metadata": {},
   "source": [
    "### Notes: If ImportError occurs, it's probably due to the huggingface-hub. \n",
    "> pip install huggingface-hub==0.25.0\n",
    "> \n",
    "> Reference: https://replicate.com/blog/how-to-prompt-llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11faa2-3cbe-4b7f-93c3-29397ed9bec9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbac4516-42ca-4dfc-9fc2-01595f351a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import os\n",
    "\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import itertools, csv, json\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"CPU\"\n",
    "HUGGING_FACE_TOKEN = os.environ.get('HUGGING_FACE_TOKEN') #in terminal: export HUGGING_FACE_TOKEN=\"YOUR_TOKEN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e9cf-f9eb-4fea-a6c7-44ec8126de3c",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f875935-7dcf-47f0-88e1-d68412712a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea66db5eb9ad4282a122026edd58759b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Set token using ENV variable\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, token\u001b[38;5;241m=\u001b[39mHUGGING_FACE_TOKEN)\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHUGGING_FACE_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m augmentation_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/modeling_utils.py:4225\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4216\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4218\u001b[0m     (\n\u001b[1;32m   4219\u001b[0m         model,\n\u001b[1;32m   4220\u001b[0m         missing_keys,\n\u001b[1;32m   4221\u001b[0m         unexpected_keys,\n\u001b[1;32m   4222\u001b[0m         mismatched_keys,\n\u001b[1;32m   4223\u001b[0m         offload_index,\n\u001b[1;32m   4224\u001b[0m         error_msgs,\n\u001b[0;32m-> 4225\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4237\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4245\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4246\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/modeling_utils.py:4728\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4724\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4725\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4726\u001b[0m                 )\n\u001b[1;32m   4727\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4728\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4732\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4734\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4735\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4740\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4741\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4742\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4744\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4746\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/modeling_utils.py:993\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    990\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 993\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/accelerate/utils/modeling.py:411\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    409\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mparam_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mold_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    413\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fp16_statistics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/parameter.py:40\u001b[0m, in \u001b[0;36mParameter.__new__\u001b[0;34m(cls, data, requires_grad)\u001b[0m\n\u001b[1;32m     36\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data) \u001b[38;5;129;01mis\u001b[39;00m Parameter:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# For ease of BC maintenance, keep this path for standard Tensor.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Eventually (tm), we should change the behavior for standard Tensor to match.\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Path for custom tensors: set a flag on the instance to indicate parameter-ness.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m t \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(requires_grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Possible Models:\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- meta-llama/Llama-3.2-3B-Instruct\n",
    "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
    "- meta-llama/Llama-3.1-70B-Instruct\n",
    "'''\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\" \n",
    "\n",
    "# Quanitisize your model dtype (for sparsity)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Set token using ENV variable\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HUGGING_FACE_TOKEN)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "augmentation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824c74d-069f-48cd-a5de-257bfa00e7ec",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29b95472-1102-4480-877e-4ccb6b170c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/dongkyu/exported_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d6e2cd-3b04-47fa-8dad-82c9ff15582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed `.txt` files from nested directories saved to finetune.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses text for LLaMA dataset.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    # Normalize whitespace and remove excessive newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace all whitespace (including newlines) with a single space\n",
    "    text = re.sub(r'\\n+', '\\n', text.strip())  # Replace multiple newlines with a single newline\n",
    "    # Remove any unwanted characters or patterns (URLs, etc.)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)  # Remove URLs\n",
    "    return text.strip()\n",
    "\n",
    "def load_txt_files_to_text_format(directory, output_file):\n",
    "    \"\"\"\n",
    "    Converts `.txt` files from a nested directory structure into a single `.txt` file with preprocessing.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Root directory containing `.txt` files (including subdirectories).\n",
    "        output_file (str): Path to save the combined and cleaned text file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_f:\n",
    "        # Recursively traverse the directory\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file_name in sorted(files):\n",
    "                if file_name.endswith('.txt'):  # Only process `.txt` files\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        cleaned_text = preprocess_text(content)\n",
    "                        # Write the cleaned text to the output file\n",
    "                        output_f.write(cleaned_text + '\\n')  # Add a newline between entries\n",
    "\n",
    "    print(f\"Processed `.txt` files from nested directories saved to {output_file}.\")\n",
    "\n",
    "# Example usage\n",
    "directory = directory_path  # Replace with your root directory with subdirectories\n",
    "output_file = \"raw_text.txt\"  # Output file path\n",
    "load_txt_files_to_text_format(directory, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43e1380e-ac80-4fe4-b115-c60592768f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file_lines(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "file_path = \"raw_text.txt\"\n",
    "lines = read_text_file_lines(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d8897c5-841e-4582-ab7c-1f3bf586a570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: (SLAM) Navigating While Mapping\\uf0c1 URL:  Section: getting_started/index.html -------------------------------------------------------------------------------- ## Overview\\uf0c1 This document explains how to use Nav2 with SLAM. The following steps show ROS 2 users how to generate occupancy grid maps and use Nav2 to move their robot around. This tutorial applies to both simulated and physical robots, but will be completed here on a physical robot. Before completing this tutorial, completing theGetting Startedis highly recommended especially if you are new to ROS and Navigation2. In this tutorial we’ll be using SLAM Toolbox. More information can be found in theROSCon talk for SLAM Toolbox ## Requirements\\uf0c1 You must install Navigation2, Turtlebot3, and SLAM Toolbox. If you don’t have them installed, please followGetting Started. SLAM Toolbox can be installed via: or from built from source in your workspace with: ## Tutorial Steps\\uf0c1 ## 0- Launch Robot Interfaces\\uf0c1 For this tutorial, we will use the turtlebot3. The turtlebot3 software can be installed via the following or on theturtlebot3 repository: If you have another robot, replace with your robot specific interfaces. Typically, this includes the robot state publisher of the URDF, simulated or physical robot interfaces, controllers, safety nodes, and the like. Run the following commands first whenever you open a new terminal during this tutorial. Launch your robot’s interface and robot state publisher, for example: ## 1- Launch Navigation2\\uf0c1 Launch Navigation without nav2_amcl and nav2_map_server. It is assumed that the SLAM node(s) will publish to /map topic and provide the map->odom transform. ## 2- Launch SLAM\\uf0c1 Bring up your choice of SLAM implementation. Make sure it provides the map->odom transform and /map topic. Run Rviz and add the topics you want to visualize such as /map, /tf, /laserscan etc. For this tutorial, we will useSLAM Toolbox. ## 3- Working with SLAM\\uf0c1 Move your robot by requesting a goal through RViz or the ROS 2 CLI, ie: You should see the map update live! To save this map to file: ## 4- Getting Started Simplification\\uf0c1 If you’re only interested in running SLAM in the turtlebot3 getting started sandbox world, we also provide a simple way to enable SLAM as a launch configuration. Rather than individually launching the interfaces, navigation, and SLAM, you can continue to use thetb3_simulation_launch.pywithslamconfig set to true. We provide the instructions above with the assumption that you’d like to run SLAM on your own robot which would have separated simulation / robot interfaces and navigation launch files that are combined intb3_simulation_launch.pyfor the purposes of easy testing. Code Examples: Language: unknown File: gitclone-b<ros2-distro>-develgit@github.com:stevemacenski/slam_toolbox.git ``` sudo apt install ros-<ros2-distro>-turtlebot3 ros-<ros2-distro>-turtlebot3-msgs ros-<ros2-distro>-turtlebot3-bringup ``` Language: unknown File: ros2launchslam_toolboxonline_async_launch.py ``` ros2 topic pub /goal_pose geometry_msgs/PoseStamped \"{header: {stamp: {sec: 0}, frame_id: \\'map\\'}, pose: {position: {x: 0.2, y: 0.0, z: 0.0}, orientation: {w: 1.0}}}\" ``` Language: unknown File: tb3_simulation_launch.py ``` ros2 launch nav2_bringup tb3_simulation_launch.py slam: = True ```\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed81e1-17dc-4a6a-9d3b-f089356f49e4",
   "metadata": {},
   "source": [
    "# Generate\n",
    "- '<|pad|>'\n",
    "- '<|begin_of_text|>'\n",
    "- '<|eot_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3e485a1-880e-4be0-be24-a1dab3bb5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instruction(note, pipe, max_new_tokens=100, num_beams=1, temperature=1.0):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\"You are a helpful assistant. Your task is to make a fine-tuning dataset. Given a ANSWER, infer the QUERY. Think carefully and write the appropriate QUERY that would derive this ANSWER. Write only the query. Example: [ANSWER] 4 [/ANSWER] [QUERY] What is 2+2? [/QUERY]\"),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"[ANSWER]: {noteA} [/ANSWER]\".format(noteA= note)\n",
    "            ),\n",
    "    }\n",
    "]\n",
    "\n",
    "    outputs = pipe(messages, max_new_tokens=max_new_tokens)\n",
    "    answer = outputs[0][\"generated_text\"][-1]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbfaa3cc-b261-4a34-ae41-116bf4a3e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instruction_answer(note, pipe, max_new_tokens=800, num_beams=1, temperature=1.0):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\"You are a helpful assistant.Your task is to make a fine-tuning dataset. Your response should be within [CORE] [/CORE], do not add explanations \\\n",
    "        Given a CONTEXT, select the most informative CORE. Example: [CONTEXT] Have you heard of ROS? The Robot Operating System (ROS) is a set of software libraries and tools for building robot applications. ROS is used in many occasions. [/CONTEXT] [CORE] Robot Operating System (ROS) is a set of software libraries and tools for building robot applications. [/CORE]\"),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"[CONTEXT]: {noteA} [/CONTEXT]\".format(noteA= note)\n",
    "            ),\n",
    "    }\n",
    "]\n",
    "\n",
    "    outputs = pipe(messages, max_new_tokens=max_new_tokens)\n",
    "    answer = outputs[0][\"generated_text\"][-1]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4129cb2-2410-41a3-8171-4c481cd3d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fine_tuning_dataset(lines, pipe, output_csv_path, max_new_tokens=100, num_beams=1, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates a fine-tuning dataset with queries and input lines, then saves it to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        lines (list): A list of text strings (answers).\n",
    "        pipe: The pipeline function for generating queries.\n",
    "        output_csv_path (str): Path to save the resulting CSV file.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        num_beams (int): Beam search parameter for the model.\n",
    "        temperature (float): Sampling temperature for the model.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for line in lines:\n",
    "        query = write_instruction(\n",
    "                note=line,\n",
    "                pipe=pipe,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_beams=num_beams,\n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        query= query['content']\n",
    "        results.append({\"query\": query, \"input\": line})\n",
    "        for _ in range(2):\n",
    "            quiz = write_instruction_answer(\n",
    "                    note=line,\n",
    "                    pipe=pipe,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=num_beams,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "    \n",
    "            quiz= quiz['content']\n",
    "            print(\"QUIZ: \", quiz)\n",
    "            query = write_instruction(\n",
    "                note=quiz,\n",
    "                pipe=pipe,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_beams=num_beams,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            query= query['content']\n",
    "            print(\"QUERY: \", query)\n",
    "            results.append({\"query\": query, \"input\": quiz})\n",
    "    # Save results to CSV\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"query\", \"input\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"Dataset saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "output_csv_path = \"generated_dataset.csv\"\n",
    "generate_fine_tuning_dataset(lines, augmentation_pipeline, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62052dc4-ab80-4239-8b79-37cf7150f3bb",
   "metadata": {},
   "source": [
    "# Make into ShareGPT Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cd370f-32ea-4093-bbe3-9a2b69dac80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('generated_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa1db21-7c96-401e-9d7d-d6ca7f23ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_tokens(df):\n",
    "    \"\"\"\n",
    "    Removes special tokens [QUERY], [/QUERY], [ANSWER], and [/ANSWER] from the 'query' and 'input' columns in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with 'query' and 'input' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with cleaned columns.\n",
    "    \"\"\"\n",
    "    # Define the list of special tokens to remove\n",
    "    special_tokens = [\"[QUERY]\", \"[/QUERY]\", \"[ANSWER]\", \"[/ANSWER]\", \"[TEXT]\", \"[/TEXT]\", \"[QUIZ]\", \"[/QUIZ]\", \"[CONTEXT]\", \"[/CONTEXT]\", \"[CORE]\", \"[/CORE]\"]\n",
    "    \n",
    "    # Remove special tokens from both columns\n",
    "    for token in special_tokens:\n",
    "        df['query'] = df['query'].str.replace(token, \"\", regex=False)\n",
    "        df['input'] = df['input'].str.replace(token, \"\", regex=False)\n",
    "    # Optionally strip leading and trailing whitespace\n",
    "    df['query'] = df['query'].str.strip()\n",
    "    df['input'] = df['input'].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c07000-c6a8-4298-ba99-52d37fc677f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = clean_special_tokens(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748bf911-de50-4413-9013-aa946047d2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of the SLAM Toolbox in ROS 2?</td>\n",
       "      <td>Title: (SLAM) Navigating While Mapping URL:  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of this document?</td>\n",
       "      <td>: This document explains how to use Nav2 with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of using Nav2 with SLAM?</td>\n",
       "      <td>: Title: (SLAM) Navigating While Mapping URL:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to load and use the STVL costmap plugin in...</td>\n",
       "      <td>Title: (STVL) Using an External Costmap Plugin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is pluginlib plugin STVL?</td>\n",
       "      <td>STVL is a demonstrative pluginlib plugin that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>What is eProsima Fast DDS?</td>\n",
       "      <td>: eProsima Fast DDS installation instructions\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>How to install eProsima Fast DDS?</td>\n",
       "      <td>eProsima Fast DDS URL:  Section: Installation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>How to install ROS 2 on macOS?</td>\n",
       "      <td>Title: macOS (source) URL:  Section: Installa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>What is the minimum macOS version supported?</td>\n",
       "      <td>: Title: macOS (source) URL:  Section: Instal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>What is the minimum macOS version supported by...</td>\n",
       "      <td>: Title: macOS (source) URL:  Section: Instal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1335 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  query  \\\n",
       "0     What is the purpose of the SLAM Toolbox in ROS 2?   \n",
       "1                 What is the purpose of this document?   \n",
       "2          What is the purpose of using Nav2 with SLAM?   \n",
       "3     How to load and use the STVL costmap plugin in...   \n",
       "4                        What is pluginlib plugin STVL?   \n",
       "...                                                 ...   \n",
       "1330                         What is eProsima Fast DDS?   \n",
       "1331                  How to install eProsima Fast DDS?   \n",
       "1332                     How to install ROS 2 on macOS?   \n",
       "1333       What is the minimum macOS version supported?   \n",
       "1334  What is the minimum macOS version supported by...   \n",
       "\n",
       "                                                  input  \n",
       "0     Title: (SLAM) Navigating While Mapping URL:  ...  \n",
       "1     : This document explains how to use Nav2 with ...  \n",
       "2     : Title: (SLAM) Navigating While Mapping URL:...  \n",
       "3     Title: (STVL) Using an External Costmap Plugin...  \n",
       "4     STVL is a demonstrative pluginlib plugin that ...  \n",
       "...                                                 ...  \n",
       "1330  : eProsima Fast DDS installation instructions\\...  \n",
       "1331  eProsima Fast DDS URL:  Section: Installation...  \n",
       "1332  Title: macOS (source) URL:  Section: Installa...  \n",
       "1333  : Title: macOS (source) URL:  Section: Instal...  \n",
       "1334  : Title: macOS (source) URL:  Section: Instal...  \n",
       "\n",
       "[1335 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41546e5c-7876-42eb-b8dd-29c4f9942044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_json(df, output_path):\n",
    "    \"\"\"\n",
    "    Save a DataFrame with 'query' and 'input' columns into a JSON file,\n",
    "    where each row is represented as a list of dictionaries for 'system', 'user', and 'assistant'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'query' and 'input' columns.\n",
    "        output_path (str): Path to save the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an helpful agent.\"},\n",
    "            {\"role\": \"user\", \"content\": row['query']},\n",
    "            {\"role\": \"assistant\", \"content\": row['input']}\n",
    "        ]\n",
    "        data.append(conversation)\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a606487b-d447-4daa-8414-48ae7bdd4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe_to_json(cleaned_df, 'output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245291b-9729-419d-98ed-e51d9c31704b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6dc439-218f-48cb-903f-bda7cd8b6007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b05f1c-2f6e-4b8d-a706-cdebdb979e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
