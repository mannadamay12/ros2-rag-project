{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a94e477-0fe1-45c2-91ee-75ee5557bdbd",
   "metadata": {},
   "source": [
    "### Notes: If ImportError occurs, it's probably due to the huggingface-hub. \n",
    "> pip install huggingface-hub==0.25.0\n",
    "> \n",
    "> Reference: https://replicate.com/blog/how-to-prompt-llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11faa2-3cbe-4b7f-93c3-29397ed9bec9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbac4516-42ca-4dfc-9fc2-01595f351a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import itertools, csv, json\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "HUGGING_FACE_TOKEN = os.environ.get('HUGGING_FACE_TOKEN') #in terminal: export HUGGING_FACE_TOKEN=\"YOUR_TOKEN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e9cf-f9eb-4fea-a6c7-44ec8126de3c",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb8f16c-3888-4661-be0e-7568de7688f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4\"  # or \"0,1\" for multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f875935-7dcf-47f0-88e1-d68412712a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1496bc84a1564ef699b6cb98913984c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Possible Models:\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- meta-llama/Llama-3.2-3B-Instruct\n",
    "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
    "- meta-llama/Llama-3.1-70B-Instruct\n",
    "'''\n",
    "#model_id = \"meta-llama/Llama-3.2-3B-Instruct\" \n",
    "model_id = \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "\n",
    "# Quanitisize your model dtype (for sparsity)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Set token using ENV variable\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HUGGING_FACE_TOKEN)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "augmentation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824c74d-069f-48cd-a5de-257bfa00e7ec",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b95472-1102-4480-877e-4ccb6b170c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/dongkyu/exported_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d6e2cd-3b04-47fa-8dad-82c9ff15582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed `.txt` files from nested directories saved to raw_text.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses text for LLaMA dataset.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    # Normalize whitespace and remove excessive newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace all whitespace (including newlines) with a single space\n",
    "    text = re.sub(r'\\n+', '\\n', text.strip())  # Replace multiple newlines with a single newline\n",
    "    # Remove any unwanted characters or patterns (URLs, etc.)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)  # Remove URLs\n",
    "    return text.strip()\n",
    "\n",
    "def load_txt_files_to_text_format(directory, output_file):\n",
    "    \"\"\"\n",
    "    Converts `.txt` files from a nested directory structure into a single `.txt` file with preprocessing.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Root directory containing `.txt` files (including subdirectories).\n",
    "        output_file (str): Path to save the combined and cleaned text file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_f:\n",
    "        # Recursively traverse the directory\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file_name in sorted(files):\n",
    "                if file_name.endswith('.txt'):  # Only process `.txt` files\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        cleaned_text = preprocess_text(content)\n",
    "                        # Write the cleaned text to the output file\n",
    "                        output_f.write(cleaned_text + '\\n')  # Add a newline between entries\n",
    "\n",
    "    print(f\"Processed `.txt` files from nested directories saved to {output_file}.\")\n",
    "\n",
    "# Example usage\n",
    "directory = directory_path  # Replace with your root directory with subdirectories\n",
    "output_file = \"raw_text.txt\"  # Output file path\n",
    "load_txt_files_to_text_format(directory, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e1380e-ac80-4fe4-b115-c60592768f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file_lines(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "file_path = \"raw_text.txt\"\n",
    "lines = read_text_file_lines(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d8897c5-841e-4582-ab7c-1f3bf586a570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: (SLAM) Navigating While Mapping\\uf0c1 URL:  Section: getting_started/index.html -------------------------------------------------------------------------------- ## Overview\\uf0c1 This document explains how to use Nav2 with SLAM. The following steps show ROS 2 users how to generate occupancy grid maps and use Nav2 to move their robot around. This tutorial applies to both simulated and physical robots, but will be completed here on a physical robot. Before completing this tutorial, completing theGetting Startedis highly recommended especially if you are new to ROS and Navigation2. In this tutorial we‚Äôll be using SLAM Toolbox. More information can be found in theROSCon talk for SLAM Toolbox ## Requirements\\uf0c1 You must install Navigation2, Turtlebot3, and SLAM Toolbox. If you don‚Äôt have them installed, please followGetting Started. SLAM Toolbox can be installed via: or from built from source in your workspace with: ## Tutorial Steps\\uf0c1 ## 0- Launch Robot Interfaces\\uf0c1 For this tutorial, we will use the turtlebot3. The turtlebot3 software can be installed via the following or on theturtlebot3 repository: If you have another robot, replace with your robot specific interfaces. Typically, this includes the robot state publisher of the URDF, simulated or physical robot interfaces, controllers, safety nodes, and the like. Run the following commands first whenever you open a new terminal during this tutorial. Launch your robot‚Äôs interface and robot state publisher, for example: ## 1- Launch Navigation2\\uf0c1 Launch Navigation without nav2_amcl and nav2_map_server. It is assumed that the SLAM node(s) will publish to /map topic and provide the map->odom transform. ## 2- Launch SLAM\\uf0c1 Bring up your choice of SLAM implementation. Make sure it provides the map->odom transform and /map topic. Run Rviz and add the topics you want to visualize such as /map, /tf, /laserscan etc. For this tutorial, we will useSLAM Toolbox. ## 3- Working with SLAM\\uf0c1 Move your robot by requesting a goal through RViz or the ROS 2 CLI, ie: You should see the map update live! To save this map to file: ## 4- Getting Started Simplification\\uf0c1 If you‚Äôre only interested in running SLAM in the turtlebot3 getting started sandbox world, we also provide a simple way to enable SLAM as a launch configuration. Rather than individually launching the interfaces, navigation, and SLAM, you can continue to use thetb3_simulation_launch.pywithslamconfig set to true. We provide the instructions above with the assumption that you‚Äôd like to run SLAM on your own robot which would have separated simulation / robot interfaces and navigation launch files that are combined intb3_simulation_launch.pyfor the purposes of easy testing. Code Examples: Language: unknown File: gitclone-b<ros2-distro>-develgit@github.com:stevemacenski/slam_toolbox.git ``` sudo apt install ros-<ros2-distro>-turtlebot3 ros-<ros2-distro>-turtlebot3-msgs ros-<ros2-distro>-turtlebot3-bringup ``` Language: unknown File: ros2launchslam_toolboxonline_async_launch.py ``` ros2 topic pub /goal_pose geometry_msgs/PoseStamped \"{header: {stamp: {sec: 0}, frame_id: \\'map\\'}, pose: {position: {x: 0.2, y: 0.0, z: 0.0}, orientation: {w: 1.0}}}\" ``` Language: unknown File: tb3_simulation_launch.py ``` ros2 launch nav2_bringup tb3_simulation_launch.py slam: = True ```\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed81e1-17dc-4a6a-9d3b-f089356f49e4",
   "metadata": {},
   "source": [
    "# Generate\n",
    "- '<|pad|>'\n",
    "- '<|begin_of_text|>'\n",
    "- '<|eot_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e485a1-880e-4be0-be24-a1dab3bb5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instruction(note, pipe, max_new_tokens=100, num_beams=1, temperature=1.0):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\"You are a helpful assistant. Your task is to make a fine-tuning dataset. Given a ANSWER, infer the QUERY. Think carefully and write the appropriate QUERY that would derive this ANSWER. Write only the query. Example: [ANSWER] 4 [/ANSWER] [QUERY] What is 2+2? [/QUERY]\"),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"[ANSWER]: {noteA} [/ANSWER]\".format(noteA= note)\n",
    "            ),\n",
    "    }\n",
    "]\n",
    "\n",
    "    outputs = pipe(messages, max_new_tokens=max_new_tokens)\n",
    "    answer = outputs[0][\"generated_text\"][-1]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbfaa3cc-b261-4a34-ae41-116bf4a3e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instruction_answer(note, pipe, max_new_tokens=800, num_beams=1, temperature=1.0):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\"You are a helpful assistant.Your task is to make a fine-tuning dataset. Your response should be within [CORE] [/CORE], do not add explanations \\\n",
    "        Given a CONTEXT, select the most informative CORE. Example: [CONTEXT] Have you heard of ROS? The Robot Operating System (ROS) is a set of software libraries and tools for building robot applications. ROS is used in many occasions. [/CONTEXT] [CORE] Robot Operating System (ROS) is a set of software libraries and tools for building robot applications. [/CORE]\"),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"[CONTEXT]: {noteA} [/CONTEXT]\".format(noteA= note)\n",
    "            ),\n",
    "    }\n",
    "]\n",
    "\n",
    "    outputs = pipe(messages, max_new_tokens=max_new_tokens)\n",
    "    answer = outputs[0][\"generated_text\"][-1]\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5292515e-e57b-45cb-95f7-2b91507ec0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instruction_qa(note, pipe, max_new_tokens=800, num_beams=1, temperature=1.0):\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\"You are a helpful assistant. Your task is to make pairs of question and answers from a given context. Only write the answers, no explanations.\\\n",
    "        Given a CONTEXT, make question-answer pairs. The answer should be provided in [Q][/Q]--[A][/A] format.\"),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"[CONTEXT]: {noteA} [/CONTEXT]\".format(noteA= note)\n",
    "            ),\n",
    "    }\n",
    "]\n",
    "\n",
    "    outputs = pipe(messages, max_new_tokens=max_new_tokens)\n",
    "    answer = outputs[0][\"generated_text\"][-1]\n",
    "    return answer\n",
    "\n",
    "def parse_qna(text):\n",
    "    \"\"\"\n",
    "    Parses text containing question and answer pairs in the format:\n",
    "    [Q] <question> [/Q]--[A] <answer> [/A]\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text containing Q&A pairs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing 'question' and 'answer'.\n",
    "    \"\"\"\n",
    "    # Define regex patterns to extract questions and answers\n",
    "    q_pattern = r\"\\[Q\\](.*?)\\[/Q\\]\"\n",
    "    a_pattern = r\"\\[A\\](.*?)\\[/A\\]\"\n",
    "    \n",
    "    # Find all question and answer matches\n",
    "    questions = re.findall(q_pattern, text, re.DOTALL)\n",
    "    answers = re.findall(a_pattern, text, re.DOTALL)\n",
    "    \n",
    "    # Combine into a list of dictionaries\n",
    "    qna_pairs = [{\"question\": q.strip(), \"answer\": a.strip()} for q, a in zip(questions, answers)]\n",
    "    \n",
    "    return qna_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4129cb2-2410-41a3-8171-4c481cd3d526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   0%|                                                                                                                                                                                                                                                              | 0/120 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Generating dataset:   1%|‚ñà‚ñà                                                                                                                                                                                                                                                  | 1/120 [00:40<1:20:55, 40.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNA:  [{'question': 'What is the recommended prerequisite for installing the webots_ros2 package?', 'answer': 'Understanding basic ROS principles covered in the beginner Tutorials, particularly Creating a workspace and Creating a package.'}, {'question': 'How do you install the webots_ros2 package from the latest up-to-date sources from Github?', 'answer': 'By running the command \"git clone --recurse-submodules src/webots_ros2\" in a terminal.'}, {'question': 'What is the purpose of the webots_ros2 package?', 'answer': 'The webots_ros2 package provides an interface between ROS 2 and Webots.'}, {'question': 'What is the environment variable used to specify the location of Webots?', 'answer': 'ROS2_WEBOTS_HOME'}, {'question': \"What happens if Webots couldn't be found in the default installation paths?\", 'answer': 'webots_ros2 will show a window offering the automatic installation of the latest compatible version of Webots.'}, {'question': 'How do you launch the webots_ros2_universal_robot example?', 'answer': 'By running the command \"ros2 launch webots_ros2_universal_robot multirobot_launch.py\"'}, {'question': 'What is the command to install the package dependencies for webots_ros2?', 'answer': 'sudo apt install python3-pip python3-rosdep python3-colcon-common-extensions'}, {'question': 'What is the command to build the package using colcon?', 'answer': 'colcon build'}, {'question': 'What is the command to source the ROS 2 environment?', 'answer': 'source /opt/ros/jazzy/setup.bash'}, {'question': 'What is the command to set the WEBOTS_HOME environment variable?', 'answer': 'export WEBOTS_HOME=/usr/local/webots'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   2%|‚ñà‚ñà‚ñà‚ñà                                                                                                                                                                                                                                                | 2/120 [01:45<1:48:22, 55.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNA:  [{'question': 'What is the goal of this tutorial?', 'answer': 'Install the webots_ros2 package and run simulation examples on Windows.'}, {'question': 'What is the recommended prerequisite for this tutorial?', 'answer': 'Understanding basic ROS principles, particularly Creating a workspace and Creating a package.'}, {'question': 'What is the webots_ros2 package?', 'answer': 'The webots_ros2 package provides an interface between ROS 2 and Webots.'}, {'question': 'What is Webots?', 'answer': 'Webots is a prerequisite to use the webots_ros2 package, and it can be installed by following the installation procedure or building it from sources.'}, {'question': 'How does ROS 2 find Webots installations?', 'answer': 'ROS 2 looks for Webots at the following locations (in this order): ROS2_WEBOTS_HOME environment variable, WEBOTS_HOME environment variable, and default installation paths for a compatible version.'}, {'question': 'What is the first task in the tutorial?', 'answer': 'Install WSL2 (Windows Subsystem for Linux).'}, {'question': 'Why is WSL2 recommended for ROS 2 on Windows?', 'answer': 'WSL2 improves the user experience with ROS 2 compared to native Windows installation, as it runs on a Linux platform.'}, {'question': 'How do you install ROS 2 in WSL?', 'answer': 'Install ROS 2 inside Ubuntu WSL, following the Ubuntu (deb packages) instructions.'}, {'question': 'How do you install webots_ros2?', 'answer': 'You can install webots_ros2 from the official released package or from the latest up-to-date sources from Github.'}, {'question': 'What is the command to install webots_ros2 distributed package?', 'answer': 'sudo apt-get install ros-jazzy-webots-ros2'}, {'question': 'How do you launch the webots_ros2_universal_robot example?', 'answer': 'First source the ROS 2 environment, then set the WEBOTS_HOME environment variable, and finally use the ROS 2 launch command to start demo packages.'}, {'question': 'What is RViz troubleshooting?', 'answer': 'RViz troubleshooting is a step to check if RViz works correctly by running any example that uses RViz.'}, {'question': 'What is the command to check if RViz works correctly?', 'answer': 'sudo apt install ros-jazzy-slam-toolbox ros2 launch webots_ros2_tiago robot_launch.py rviz:=true slam:=true'}, {'question': 'How do you enable X11 forwarding in WSL?', 'answer': 'Install VcXsrv, launch VcXsrv, and export the DISPLAY variable in your WSL environment.'}, {'question': 'What is the command to export the DISPLAY variable?', 'answer': \"export DISPLAY=$(ip route list default | awk '{print $3}'):0\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   2%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                                                                                                                                              | 3/120 [02:43<1:49:28, 56.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNA:  [{'question': 'What is the goal of this tutorial?', 'answer': 'Install the webots_ros2 package and run simulation examples on macOS.'}, {'question': 'What is the recommended tutorial level for this tutorial?', 'answer': 'Advanced'}, {'question': 'How long will it take to complete this tutorial?', 'answer': '10 minutes'}, {'question': 'What is the webots_ros2 package?', 'answer': 'The webots_ros2 package provides an interface between ROS 2 and Webots.'}, {'question': 'What is necessary to install in order to use the webots_ros2 package in the virtual machine?', 'answer': 'Webots should be installed natively on macOS.'}, {'question': 'What is the solution based on UTM virtual machines?', 'answer': 'It provides an improved user experience with ROS 2 compared to native macOS installation, as it runs ROS in a Linux environment.'}, {'question': 'What is the purpose of the shared folder?', 'answer': 'The shared folder allows the script to transfer the world and other resource files from the VM to macOS where Webots is running.'}, {'question': 'How to create a new image in UTM software?', 'answer': 'Select the ISO image in the Boot ISO Image field, leave all hardware settings at default, select a shared folder, and leave all the remaining parameters as default.'}, {'question': 'What to do during the first launch of the VM?', 'answer': 'Install Ubuntu and choose a username for your account.'}, {'question': 'How to configure the VM?', 'answer': 'Install ROS 2, create a shared folder, and mount it to the host.'}, {'question': 'What environment variable must always be set in order for the package to work properly in the VM?', 'answer': 'WEBOTS_SHARED_FOLDER'}, {'question': 'How to install webots_ros2?', 'answer': 'Either install webots_ros2 from the official released package, or install it from the latest up-to-date sources from Github.'}, {'question': 'How to launch the webots_ros2_universal_robot example?', 'answer': 'Run a local TCP simulation server, source the ROS 2 environment, set the WEBOTS_SHARED_FOLDER environment variable, and use the ROS 2 launch command to start the demo package.'}, {'question': 'What to do if Webots is closed or the ROS 2 process is interrupted?', 'answer': 'The local server will automatically wait for a new package launch and the shared folder will be cleaned for the next run.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   3%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                                                                                                                                           | 4/120 [04:48<2:41:15, 83.41s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNA:  [{'question': 'How to enable multicast for DDS communication?', 'answer': 'Enable multicast by running `ros2 multicast receive` in one terminal and `ros2 multicast send` in another. If no response is received, update the firewall configuration to allow multicast using `ufw`.'}, {'question': 'How to verify if the multicast flag is enabled for the network interface?', 'answer': 'Use the `ifconfig` tool and look for `MULTICAST` in the flags section.'}, {'question': 'What to do if `rclpy` fails to import due to missing C extension libraries?', 'answer': 'Compare the libraries present in the directory with the one mentioned in the error message and ensure that the same Python interpreter is used as the one used to build the binary.'}, {'question': 'How to fix the internal compiler error on a memory-constrained platform like Raspberry PI?', 'answer': 'Build single-threaded by prefixing the build invocation with `MAKEFLAGS=-j1`.'}, {'question': 'What to do if `ros1_bridge` requires 4Gb of free RAM to compile but the system does not have enough RAM?', 'answer': 'Use `COLCON_IGNORE` in that folder and skip its compilation.'}, {'question': 'How to avoid multiple host interference when running multiple instances on the same network?', 'answer': 'Set the environment variable `ROS_DOMAIN_ID` to a different integer, the default is zero.'}, {'question': 'What to do if exceptions occur when trying to source the environment after building from source?', 'answer': 'Upgrade `colcon` related packages using `colcon` and check for newer versions available.'}, {'question': 'Why does `rviz2` fail to start on a Wayland display system?', 'answer': 'Due to an incompatibility between Wayland and RViz2. Run RViz2 in X11 compatibility mode using `QT_QPA_PLATFORM=xcb`.'}, {'question': 'How to fix the segmentation fault when using `pyenv` on macOS?', 'answer': 'Build Python with Frameworks enabled on macOS when using `pyenv`.'}, {'question': 'What to do if library loading issues occur at runtime on macOS?', 'answer': 'Disable System Integrity Protection (SIP) by following the instructions.'}, {'question': \"How to fix the Qt build error: unknown type name 'Q_ENUM'?\", 'answer': 'Ensure that qt5 is installed instead of qt4.'}, {'question': 'What to do if `opencv` is installed and causes issues with `libjpeg`, `libtiff`, and `libpng`?', 'answer': 'Unlink `libpng`, `libtiff`, and `libjpeg` using `brew unlink` and update `opencv` using `brew reinstall`.'}, {'question': \"How to fix the Xcode-select error: tool 'xcodebuild' requires Xcode, but active developer directory is a command line instance?\", 'answer': 'Install the command line tool using `xcode-select --install`, accept the terms and conditions of Xcode, and point `xcode-select` to the Xcode app Developer directory.'}, {'question': 'What to do if `rosdep` fails to install Qt5 on macOS?', 'answer': 'Run `cd /usr/local/Cellar` and `sudo ln -s qt qt5` to resolve the linking issue.'}, {'question': 'How to fix the import failing issue even with the library present on the system on Windows?', 'answer': 'Install all dependencies listed in the \"Installing prerequisites\" sections of the installation instructions and ensure that the same version of dependencies is used as those used to build the binaries.'}, {'question': 'How to fix the CMake error setting modification time on Windows?', 'answer': 'Exclude the workspace location from Windows Defender or anti-virus software to prevent it from scanning those files.'}, {'question': 'What to do if the 260 character path limit is exceeded on Windows?', 'answer': 'Enable long paths by running `regedit.exe`, navigating to `Computer\\\\HKEY_LOCAL_MACHINE\\\\SYSTEM\\\\CurrentControlSet\\\\Control\\\\FileSystem`, and setting `LongPathsEnabled` to `0x00000001 (1)`. Alternatively, use the Group Policy Editor to enable long paths.'}, {'question': 'How to fix the CMake packages unable to find `asio`, `tinyxml2`, `tinyxml`, or `eigen` issue on Windows?', 'answer': 'Uninstall and reinstall the chocolatey packages for `asio`, `tinyxml2`, etc.'}, {'question': 'What to do if `patch.exe` opens a new command window and asks for administrator on Windows?', 'answer': 'Uninstall `patch` using `choco uninstall patch` and then reinstall it.'}, {'question': 'How to fix the failed to load Fast RTPS shared library issue on Windows?', 'answer': 'Install the Visual C++ Redistributable Packages for Visual Studio 2013, which includes `msvcr20.dll`.'}, {'question': 'What to do if a ROS binary fails to create a process on Windows?', 'answer': 'Ensure that the Python interpreter is available under the expected path (default: `C:\\\\Python38\\\\`).'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   4%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                                                                                                                                         | 5/120 [05:15<2:00:45, 63.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNA:  [{'question': 'What is the recommended method for installing ROS 2 on Linux?', 'answer': 'Installing from packages'}, {'question': 'What is the difference between installing from binary packages and building from source?', 'answer': 'Binary packages are for general use and provide an already-built install of ROS 2, while building from source is meant for developers looking to alter or explicitly omit parts of ROS 2‚Äôs base'}, {'question': 'Which platforms are supported for building ROS 2 from source?', 'answer': 'Ubuntu Linux 22.04, Windows 10, RHEL-9/Fedora, macOS'}, {'question': 'What is the recommended method for installing ROS 2 on Windows?', 'answer': 'Binary archive'}, {'question': 'Why is installing from packages recommended on Linux?', 'answer': 'It installs necessary dependencies automatically and also updates alongside regular system updates'}, {'question': \"What should you do if you don't have root access on Linux?\", 'answer': 'Use the binary archive'}, {'question': 'What should you do if you plan to contribute directly to ROS 2 core packages?', 'answer': 'Install the latest development from source'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                                                                                                                                       | 6/120 [06:17<1:58:58, 62.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNA:  [{'question': 'What is the purpose of the interface definition language (IDL) in ROS 2?', 'answer': 'To describe interfaces and automatically generate source code for the interface type in several target languages.'}, {'question': 'What are the three types of interfaces used in ROS applications?', 'answer': 'Topics, services, or actions.'}, {'question': 'What is the purpose of.msg files in ROS?', 'answer': 'To describe the fields of a ROS message.'}, {'question': 'What is the format of a field definition in a.msg file?', 'answer': 'fieldtype fieldname'}, {'question': 'What are the two parts of a.srv file?', 'answer': 'A request and a response.'}, {'question': 'What is the purpose of constants in ROS?', 'answer': 'To define a value that can never be changed programmatically.'}, {'question': 'What is the format of a constant definition in ROS?', 'answer': 'constanttype CONSTANTNAME=constantvalue'}, {'question': 'What is the purpose of actions in ROS?', 'answer': 'To provide a long-running request/response communication.'}, {'question': 'What are the three parts of an action definition in ROS?', 'answer': 'Request fields, response fields, and feedback fields.'}, {'question': 'What is the format of an action definition in ROS?', 'answer': '<request_type><request_fieldname>---<response_type><response_fieldname>---<feedback_type><feedback_fieldname>'}, {'question': 'Can default values be set to any field in a message type?', 'answer': 'No, default values are not supported for string arrays and complex types.'}, {'question': 'How are default values defined in a message type?', 'answer': 'By adding a third element to the field definition line, i.e: fieldtype fieldname fielddefaultvalue'}, {'question': 'What are the rules for field names in ROS?', 'answer': 'Field names must be lowercase alphanumeric characters with underscores for separating words, start with an alphabetic character, and not end with an underscore or have two consecutive underscores.'}, {'question': 'What are the built-in types supported in ROS?', 'answer': 'bool, byte, char, float32, float64, int8, int16, int32, int64, string, uint8, uint16, uint32, uint64, and wstring.'}, {'question': 'Can built-in types be used to define arrays?', 'answer': 'Yes, every built-in-type can be used to define arrays.'}, {'question': 'What are the rules for constant names in ROS?', 'answer': 'Constant names must be uppercase.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset:   5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                                                                                                                                       | 6/120 [06:23<2:01:28, 63.94s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 208.31 MiB is free. Including non-PyTorch memory, this process has 47.30 GiB memory in use. Of the allocated memory 45.30 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     55\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mgenerate_fine_tuning_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmentation_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m, in \u001b[0;36mgenerate_fine_tuning_dataset\u001b[0;34m(lines, pipe, output_csv_path, max_new_tokens, num_beams, temperature)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mquery = write_instruction(\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m        note=line,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mresults.append({\"query\": query, \"input\": line})\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m     qa \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_instruction_qa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     qa\u001b[38;5;241m=\u001b[39m qa[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     41\u001b[0m     qna_pairs\u001b[38;5;241m=\u001b[39m parse_qna(qa)\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mwrite_instruction_qa\u001b[0;34m(note, pipe, max_new_tokens, num_beams, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_instruction_qa\u001b[39m(note, pipe, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m      2\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m ]\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     answer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:267\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    263\u001b[0m     text_inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, KeyDataset) \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    264\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         chats \u001b[38;5;241m=\u001b[39m [Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs]  \u001b[38;5;66;03m# üêà üêà üêà\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/pipelines/base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/pipelines/base.py:1309\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/pipelines/base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:484\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    481\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    483\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    512\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 208.31 MiB is free. Including non-PyTorch memory, this process has 47.30 GiB memory in use. Of the allocated memory 45.30 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def generate_fine_tuning_dataset(lines, pipe, output_csv_path, max_new_tokens=1200, num_beams=1, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates a fine-tuning dataset with queries and input lines, then saves it to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        lines (list): A list of text strings (answers).\n",
    "        pipe: The pipeline function for generating queries.\n",
    "        output_csv_path (str): Path to save the resulting CSV file.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        num_beams (int): Beam search parameter for the model.\n",
    "        temperature (float): Sampling temperature for the model.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    #for line in lines:\n",
    "    lines = lines[325:]\n",
    "    for line in tqdm(lines, desc=\"Generating dataset\"):\n",
    "        torch.cuda.empty_cache()\n",
    "        '''\n",
    "        query = write_instruction(\n",
    "                note=line,\n",
    "                pipe=pipe,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                num_beams=num_beams,\n",
    "                temperature=temperature\n",
    "            )\n",
    "\n",
    "        query= query['content']\n",
    "        results.append({\"query\": query, \"input\": line})\n",
    "        '''\n",
    "        for _ in range(1):\n",
    "            qa = write_instruction_qa(\n",
    "                    note=line,\n",
    "                    pipe=pipe,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    num_beams=num_beams,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "            \n",
    "            qa= qa['content']\n",
    "            qna_pairs= parse_qna(qa)\n",
    "            \n",
    "            print(\"QNA: \", qna_pairs)\n",
    "            for qna in qna_pairs:\n",
    "                results.append({\"query\": qna['question'], \"input\": qna['answer']})\n",
    "    # Save results to CSV\n",
    "    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"query\", \"input\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"Dataset saved to {output_csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "output_csv_path = \"generated_dataset.csv\"\n",
    "generate_fine_tuning_dataset(lines, augmentation_pipeline, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62052dc4-ab80-4239-8b79-37cf7150f3bb",
   "metadata": {},
   "source": [
    "# Make into ShareGPT Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f51e0f-3fbd-4c0a-8859-98f447f26fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old= pd.read_csv('generated_dataset_early.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25cd370f-32ea-4093-bbe3-9a2b69dac80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('generated_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9df62dbb-57a1-41bf-add0-d8df115f8064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the recommended prerequisite for this ...</td>\n",
       "      <td>Completing the Getting Started tutorial, espec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What toolbox is used in this tutorial for SLAM?</td>\n",
       "      <td>SLAM Toolbox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the command to install SLAM Toolbox?</td>\n",
       "      <td>sudo apt install ros-&lt;ros2-distro&gt;-slam-toolbo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What launch file is used to bring up the turtl...</td>\n",
       "      <td>tb3_simulation_launch.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the command to launch Navigation2 with...</td>\n",
       "      <td>ros2 launch nav2_bringup navigation.launch.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>What is the Rolling distribution of ROS 2?</td>\n",
       "      <td>The Rolling distribution of ROS 2 is the rolli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>How often is a new ROS 2 distribution released?</td>\n",
       "      <td>A new ROS 2 distribution is released yearly on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>What is the difference between the Rolling dis...</td>\n",
       "      <td>The Rolling distribution is continuously updat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>What happens to packages released into the Rol...</td>\n",
       "      <td>Packages released into the Rolling distributio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>What is the recommended distribution for most ...</td>\n",
       "      <td>The most recent stable distribution is recomme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1896 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  query  \\\n",
       "0     What is the recommended prerequisite for this ...   \n",
       "1       What toolbox is used in this tutorial for SLAM?   \n",
       "2          What is the command to install SLAM Toolbox?   \n",
       "3     What launch file is used to bring up the turtl...   \n",
       "4     What is the command to launch Navigation2 with...   \n",
       "...                                                 ...   \n",
       "1891         What is the Rolling distribution of ROS 2?   \n",
       "1892    How often is a new ROS 2 distribution released?   \n",
       "1893  What is the difference between the Rolling dis...   \n",
       "1894  What happens to packages released into the Rol...   \n",
       "1895  What is the recommended distribution for most ...   \n",
       "\n",
       "                                                  input  \n",
       "0     Completing the Getting Started tutorial, espec...  \n",
       "1                                          SLAM Toolbox  \n",
       "2     sudo apt install ros-<ros2-distro>-slam-toolbo...  \n",
       "3                              tb3_simulation_launch.py  \n",
       "4         ros2 launch nav2_bringup navigation.launch.py  \n",
       "...                                                 ...  \n",
       "1891  The Rolling distribution of ROS 2 is the rolli...  \n",
       "1892  A new ROS 2 distribution is released yearly on...  \n",
       "1893  The Rolling distribution is continuously updat...  \n",
       "1894  Packages released into the Rolling distributio...  \n",
       "1895  The most recent stable distribution is recomme...  \n",
       "\n",
       "[1896 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fa1db21-7c96-401e-9d7d-d6ca7f23ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_tokens(df):\n",
    "    \"\"\"\n",
    "    Removes special tokens [QUERY], [/QUERY], [ANSWER], and [/ANSWER] from the 'query' and 'input' columns in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with 'query' and 'input' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with cleaned columns.\n",
    "    \"\"\"\n",
    "    # Define the list of special tokens to remove\n",
    "    special_tokens = [\"[QUERY]\", \"[/QUERY]\", \"[ANSWER]\", \"[/ANSWER]\", \"[TEXT]\", \"[/TEXT]\", \"[QUIZ]\", \"[/QUIZ]\", \"[CONTEXT]\", \"[/CONTEXT]\", \"[CORE]\", \"[/CORE]\"]\n",
    "    \n",
    "    # Remove special tokens from both columns\n",
    "    for token in special_tokens:\n",
    "        df['query'] = df['query'].str.replace(token, \"\", regex=False)\n",
    "        df['input'] = df['input'].str.replace(token, \"\", regex=False)\n",
    "    # Optionally strip leading and trailing whitespace\n",
    "    df['query'] = df['query'].str.strip()\n",
    "    df['input'] = df['input'].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53c07000-c6a8-4298-ba99-52d37fc677f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = clean_special_tokens(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "748bf911-de50-4413-9013-aa946047d2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the recommended prerequisite for this ...</td>\n",
       "      <td>Completing the Getting Started tutorial, espec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What toolbox is used in this tutorial for SLAM?</td>\n",
       "      <td>SLAM Toolbox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the command to install SLAM Toolbox?</td>\n",
       "      <td>sudo apt install ros-&lt;ros2-distro&gt;-slam-toolbo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What launch file is used to bring up the turtl...</td>\n",
       "      <td>tb3_simulation_launch.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the command to launch Navigation2 with...</td>\n",
       "      <td>ros2 launch nav2_bringup navigation.launch.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>What is the Rolling distribution of ROS 2?</td>\n",
       "      <td>The Rolling distribution of ROS 2 is the rolli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>How often is a new ROS 2 distribution released?</td>\n",
       "      <td>A new ROS 2 distribution is released yearly on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>What is the difference between the Rolling dis...</td>\n",
       "      <td>The Rolling distribution is continuously updat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>What happens to packages released into the Rol...</td>\n",
       "      <td>Packages released into the Rolling distributio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>What is the recommended distribution for most ...</td>\n",
       "      <td>The most recent stable distribution is recomme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1896 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  query  \\\n",
       "0     What is the recommended prerequisite for this ...   \n",
       "1       What toolbox is used in this tutorial for SLAM?   \n",
       "2          What is the command to install SLAM Toolbox?   \n",
       "3     What launch file is used to bring up the turtl...   \n",
       "4     What is the command to launch Navigation2 with...   \n",
       "...                                                 ...   \n",
       "1891         What is the Rolling distribution of ROS 2?   \n",
       "1892    How often is a new ROS 2 distribution released?   \n",
       "1893  What is the difference between the Rolling dis...   \n",
       "1894  What happens to packages released into the Rol...   \n",
       "1895  What is the recommended distribution for most ...   \n",
       "\n",
       "                                                  input  \n",
       "0     Completing the Getting Started tutorial, espec...  \n",
       "1                                          SLAM Toolbox  \n",
       "2     sudo apt install ros-<ros2-distro>-slam-toolbo...  \n",
       "3                              tb3_simulation_launch.py  \n",
       "4         ros2 launch nav2_bringup navigation.launch.py  \n",
       "...                                                 ...  \n",
       "1891  The Rolling distribution of ROS 2 is the rolli...  \n",
       "1892  A new ROS 2 distribution is released yearly on...  \n",
       "1893  The Rolling distribution is continuously updat...  \n",
       "1894  Packages released into the Rolling distributio...  \n",
       "1895  The most recent stable distribution is recomme...  \n",
       "\n",
       "[1896 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41546e5c-7876-42eb-b8dd-29c4f9942044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_to_json(df, output_path):\n",
    "    \"\"\"\n",
    "    Save a DataFrame with 'query' and 'input' columns into a JSON file,\n",
    "    where each row is represented as a list of dictionaries for 'system', 'user', and 'assistant'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'query' and 'input' columns.\n",
    "        output_path (str): Path to save the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        print(type(row['input']))\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an helpful agent.\"},\n",
    "            {\"role\": \"user\", \"content\": row['query']},\n",
    "            {\"role\": \"assistant\", \"content\": row['input']}\n",
    "        ]\n",
    "        data.append(conversation)\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4819d17d-be6e-429f-93b0-b204ab9ee02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "def save_dataframe_to_json(df, output_path):\n",
    "    \"\"\"\n",
    "    Save a DataFrame with 'query' and 'input' columns into a JSON file,\n",
    "    where each row is represented as a list of dictionaries for 'system', 'user', and 'assistant'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'query' and 'input' columns.\n",
    "        output_path (str): Path to save the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Ensure 'query' and 'input' are valid JSON-compatible values\n",
    "        query = row['query'] if not (isinstance(row['query'], float) and math.isnan(row['query'])) else None\n",
    "        user_input = row['input'] if not (isinstance(row['input'], float) and math.isnan(row['input'])) else None\n",
    "\n",
    "        # Construct the conversation\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful agent.\"},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            {\"role\": \"assistant\", \"content\": user_input}\n",
    "        ]\n",
    "        data.append(conversation)\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "# df = pd.DataFrame({'query': [\"Hello\", \"How are you?\"], 'input': [\"Hi there!\", None]})\n",
    "# save_dataframe_to_json(df, 'output.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a606487b-d447-4daa-8414-48ae7bdd4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataframe_to_json(cleaned_df, 'output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245291b-9729-419d-98ed-e51d9c31704b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6dc439-218f-48cb-903f-bda7cd8b6007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b05f1c-2f6e-4b8d-a706-cdebdb979e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
