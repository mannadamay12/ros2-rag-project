{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a94e477-0fe1-45c2-91ee-75ee5557bdbd",
   "metadata": {},
   "source": [
    "### Notes: If ImportError occurs, it's probably due to the huggingface-hub. \n",
    "> pip install huggingface-hub==0.25.0\n",
    "\n",
    "\n",
    "### Reference: https://medium.com/@hakeemsyd/how-to-fine-tune-your-llama-3-2-model-49a6f8c7621a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11faa2-3cbe-4b7f-93c3-29397ed9bec9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbac4516-42ca-4dfc-9fc2-01595f351a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM,HfArgumentParser\n",
    "from transformers import Trainer, TrainingArguments, TextStreamer, logging\n",
    "\n",
    "from peft import LoraConfig,PeftModel,prepare_model_for_kbit_training,get_peft_model\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "import re, json\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#DEVICE = \"cpu\"\n",
    "HUGGING_FACE_TOKEN = os.environ.get('HUGGING_FACE_TOKEN') #in terminal: export HUGGING_FACE_TOKEN=\"YOUR_TOKEN\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6005b05e-da4c-4666-a188-fd47ba101555",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # or \"0,1\" for multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e9cf-f9eb-4fea-a6c7-44ec8126de3c",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f875935-7dcf-47f0-88e1-d68412712a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7be9bf3e5d42d197eec3634c8f31ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Possible Models:\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- meta-llama/Llama-3.2-3B-Instruct\n",
    "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
    "'''\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\" \n",
    "\n",
    "# Quantize your model dtype (for sparsity)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Set token using ENV variable\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HUGGING_FACE_TOKEN)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    #quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b5d328c-cb75-4e60-a7b5-2fbafebe8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<|pad|>'\n",
      "'<|begin_of_text|>'\n",
      "'<|eot_id|>'\n"
     ]
    }
   ],
   "source": [
    "print(repr(tokenizer.pad_token)) ## None\n",
    "print(repr(tokenizer.bos_token)) ## ''\n",
    "print(repr(tokenizer.eos_token)) ## ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4baf2d-eb9d-4c05-badc-7bf74ee3753d",
   "metadata": {},
   "source": [
    "# Fine-tuning with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4654279-7e81-4a72-b1ce-c2dc8c501ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=8, lora_alpha=8, #16,16 \n",
    "                         target_modules= ['q_proj','k_proj','v_proj'],\n",
    "                         lora_dropout= 0.1)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.gradient_checkpointing_enable() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79617ec0-e6f1-499a-9613-0fddaa1c7a4b",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2ca899f-76b2-4adf-84fc-cd8a2bfb5ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/dongkyu/exported_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d66faef6-0b4d-4afc-9ce5-f5adb2fd70bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed `.txt` files from nested directories saved to finetune.txt.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses text for LLaMA dataset.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    # Normalize whitespace and remove excessive newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace all whitespace (including newlines) with a single space\n",
    "    text = re.sub(r'\\n+', '\\n', text.strip())  # Replace multiple newlines with a single newline\n",
    "    # Remove any unwanted characters or patterns (URLs, etc.)\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)  # Remove URLs\n",
    "    return text.strip()\n",
    "\n",
    "def load_txt_files_to_llama_format(directory, output_file):\n",
    "    \"\"\"\n",
    "    Converts `.txt` files from a nested directory structure to LLaMA-compatible JSONL format with preprocessing.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Root directory containing `.txt` files (including subdirectories).\n",
    "        output_file (str): Path to save the output JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_f:\n",
    "        # Recursively traverse the directory\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file_name in sorted(files):\n",
    "                if file_name.endswith('.txt'):  # Only process `.txt` files\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        cleaned_text = preprocess_text(content)\n",
    "                        # Create JSON object and write to file\n",
    "                        json_obj = {\"text\": cleaned_text}\n",
    "                        output_f.write(json.dumps(json_obj, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"Processed `.txt` files from nested directories saved to {output_file} in JSONL format.\")\n",
    "\n",
    "# Example usage\n",
    "directory = directory_path  # Your root directory with subdirectories\n",
    "output_file = \"finetune.jsonl\"  # Output file path\n",
    "load_txt_files_to_llama_format(directory, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e3cfc-e735-49d5-87e1-d5388d1d001b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2faa6a95-3096-401a-8e08-8b73df9f0ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586ae213d4dc42958a71349c7c9da4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files = \"finetune.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447de6b5-875a-407d-93d2-d4d551557bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512, padding= \"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7693c9-a544-4855-bd98-21b5cedaeddc",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1e3c12e-525b-4aa7-8c57-567a5902472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 1,236,994,048 || trainable%: 0.09536408052304549\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f42c0e-0c28-43c8-879a-caee7832baa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args= TrainingArguments(\n",
    "    output_dir= \"./results\",\n",
    "    per_device_train_batch_size= 4,\n",
    "    per_device_eval_batch_size= 4,\n",
    "    num_train_epochs= 3,\n",
    "    learning_rate= 2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    #tokenized_train_dataset\n",
    "    #train_dataset= tokenized_datasets['train'],\n",
    "    #eval_dataset= tokenized_datasets['train']\n",
    ")\n",
    "\n",
    "trainer= Trainer(\n",
    "    model=model,\n",
    "    args= training_args,\n",
    "    train_dataset= tokenized_train_dataset\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9811883-493a-4377-a684-29b0b30a2354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283436c-e273-46ad-8898-dc7039070ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
