{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a94e477-0fe1-45c2-91ee-75ee5557bdbd",
   "metadata": {},
   "source": [
    "### Notes: If ImportError occurs, it's probably due to the huggingface-hub. \n",
    "> pip install huggingface-hub==0.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11faa2-3cbe-4b7f-93c3-29397ed9bec9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbac4516-42ca-4dfc-9fc2-01595f351a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"CPU\"\n",
    "DEVICE = \"cpu\"\n",
    "HUGGING_FACE_TOKEN = os.environ.get('HUGGING_FACE_TOKEN') #in terminal: export HUGGING_FACE_TOKEN=\"YOUR_TOKEN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e9cf-f9eb-4fea-a6c7-44ec8126de3c",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f875935-7dcf-47f0-88e1-d68412712a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a39c3ed32794991be19a746b9a3cc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Possible Models:\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- meta-llama/Llama-3.2-3B-Instruct\n",
    "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
    "'''\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\" \n",
    "\n",
    "# Quanitisize your model dtype (for sparsity)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Set token using ENV variable\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HUGGING_FACE_TOKEN)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea42f217-eb0c-4583-ade5-5832bbd9c7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongkyu/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/dongkyu/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI! I'm so excited to share my latest project with you all! I've been working on a new line of eco-friendly, cruelty-free, and vegan-friendly makeup products, and I just can't wait to show you all the gorgeous shades and formulas I\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"HI!\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "inputs = inputs.to('cpu')  # Ensure inputs are on CPU\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bec46-9f0d-4994-8d74-c1624b82c354",
   "metadata": {},
   "source": [
    "### Knowledge Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde179ee-3690-4ec2-a867-c0aa68a42d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdf base\n",
    "pdf_base_path = '/home/dongkyu/RAG/knowledge'\n",
    "loader = PyPDFDirectoryLoader(pdf_base_path) #The Knowledge Base Folder\n",
    "pdf_docs = loader.load()\n",
    "len(pdf_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9f652e-2267-48d6-8a03-fd19df4e4c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knowledge base\n",
    "knowledge_base_path= \"/home/dongkyu/exported_docs\"\n",
    "txt_loader = DirectoryLoader(knowledge_base_path, glob=\"**/*.txt\")\n",
    "docs = txt_loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e5b6c9-ce55-4694-9059-ebd546902055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2929"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154fafe3-7dd3-4303-a8de-38353a25998c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-base\",\n",
    "    model_kwargs={\"device\": DEVICE}\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dd4e7-d957-4780-9cd9-b6cbc1f79590",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e919ab43-cb51-4c54-8404-1d1bedd9c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Based on the information in this document provided in context, answer the question as accurately as possible in 1 or 2 lines. If the information is not in the context,\n",
    "respond with \"I don't know\" or a similar acknowledgment that the answer is not available.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. Do not provide commentary or elaboration more than 1 or 2 lines.?\"\n",
    "\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8952142d-ec91-4c67-9aa0-b9a71910fe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2720603/3144239959.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac8e41b5-cb60-4215-a297-400325190496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2720603/1986214230.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = ask(\"Write a sample code for Bouncy Bolson\")\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Here is a simple implementation in Python:\n",
      "```\n",
      "class BouncyBolsonNode:\n",
      "    def __init__(self, value):\n",
      "        self.value = value\n",
      "\n",
      "    def bounce(self):\n",
      "        return self.value * 2\n",
      "\n",
      "    def bouncify(self):\n",
      "        if self.bounce() > 10:\n",
      "            print(\"BOUNCE!\")\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "# Create nodes and start bouncing\n",
      "node1 = BouncyBolsonNode(5)\n",
      "node2 = BouncyBolsonNode(3)\n",
      "\n",
      "print(node1.bounce()) # prints 10\n",
      "print(node1.bouncify()) # prints \"BOUNCE!\" and returns True\n",
      "print(node2.bounce()) # prints 6\n",
      "print(node2.bouncify()) # prints nothing (False) because it's less than 10\n",
      "```\n",
      "\n",
      "Note that this example does not include error checking or handling.\n",
      "\n",
      "Please note that I have created an entirely new class `BouncyBolsonNode` instead of modifying existing classes as per your request.\n",
      "\n",
      "\n",
      "## Step 1: Define the problem and identify key requirements.\n",
      "The task requires creating a custom node-based system inspired by the concept of a \"bouncy bolson,\" which seems to be related to data compression or encoding but lacks clear definitions in the provided context. We will assume the goal is to create a basic structure for such a system, focusing on implementing a \"bounce\" function and a \"bouncify\" method within a custom node class.\n",
      "\n",
      "## Step 2: Design the Node Class Structure.\n",
      "To implement the required functionality, we need to define a class with attributes like `value`, methods like `bounce()` and `bouncify()`, and possibly additional features based on the specific needs of our \"bouncy bolson.\"\n",
      "\n",
      "## Step 3: Implement the Bounce Functionality.\n",
      "Within the `bounce()` method, we should multiply the current `value` attribute by some factor (e.g., 2), simulating the effect of \"bouncing.\" This could represent an increase in size or complexity when certain conditions are met.\n",
      "\n",
      "## Step 4: Develop the Bouncify Method.\n",
      "For the `bouncify()` method, we'll check whether the result of calling `bounce()` exceeds a threshold (in this case, 10). If so, we trigger a notification (\"BOUNCE!\") and potentially modify the object state accordingly. Otherwise, we simply return a boolean indicating failure to meet the condition.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "result = ask(\"Write a sample code for Bouncy Bolson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066187c-0111-4d9f-9d0d-5defa348cfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4654279-7e81-4a72-b1ce-c2dc8c501ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a8d90-ac6f-4e70-82c9-439df352c7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
