{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a94e477-0fe1-45c2-91ee-75ee5557bdbd",
   "metadata": {},
   "source": [
    "### Notes: If ImportError occurs, it's probably due to the huggingface-hub. \n",
    "> pip install huggingface-hub==0.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11faa2-3cbe-4b7f-93c3-29397ed9bec9",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbac4516-42ca-4dfc-9fc2-01595f351a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, TextStreamer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"CPU\"\n",
    "DEVICE = \"cpu\"\n",
    "HUGGING_FACE_TOKEN = os.environ.get('HUGGING_FACE_TOKEN') #in terminal: export HUGGING_FACE_TOKEN=\"YOUR_TOKEN\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d5e9cf-f9eb-4fea-a6c7-44ec8126de3c",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f875935-7dcf-47f0-88e1-d68412712a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466015f5be0749688def1b8ddc748d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Possible Models:\n",
    "- meta-llama/Llama-3.2-1B-Instruct\n",
    "- meta-llama/Llama-3.2-3B-Instruct\n",
    "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
    "'''\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\" \n",
    "\n",
    "# Quanitisize your model dtype (for sparsity)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Set token using ENV variable\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HUGGING_FACE_TOKEN)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea42f217-eb0c-4583-ade5-5832bbd9c7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongkyu/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/dongkyu/miniconda3/envs/lm/lib/python3.12/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI! I'm so excited to share my latest project with you all! I've been working on a new line of eco-friendly, cruelty-free, and vegan-friendly makeup products, and I just can't wait to show you all the gorgeous shades and formulas I\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"HI!\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "inputs = inputs.to('cpu')  # Ensure inputs are on CPU\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    num_beams=1,\n",
    "    do_sample=False,\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bec46-9f0d-4994-8d74-c1624b82c354",
   "metadata": {},
   "source": [
    "### Knowledge Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde179ee-3690-4ec2-a867-c0aa68a42d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdf base\n",
    "pdf_base_path = '/home/dongkyu/RAG/knowledge'\n",
    "loader = PyPDFDirectoryLoader(pdf_base_path) #The Knowledge Base Folder\n",
    "pdf_docs = loader.load()\n",
    "len(pdf_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c9f652e-2267-48d6-8a03-fd19df4e4c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knowledge base\n",
    "knowledge_base_path= \"/home/dongkyu/exported_docs\"\n",
    "txt_loader = DirectoryLoader(knowledge_base_path, glob=\"**/*.txt\")\n",
    "docs = txt_loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e5b6c9-ce55-4694-9059-ebd546902055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3905"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154fafe3-7dd3-4303-a8de-38353a25998c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-base\",\n",
    "    model_kwargs={\"device\": DEVICE}\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660dd4e7-d957-4780-9cd9-b6cbc1f79590",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e919ab43-cb51-4c54-8404-1d1bedd9c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
    "Based on the information in this document provided in context, answer the question as accurately as possible in 1 or 2 lines. If the information is not in the context,\n",
    "respond with \"I don't know\" or a similar acknowledgment that the answer is not available.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    return f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "\"\"\".strip()\n",
    "\n",
    "SYSTEM_PROMPT = \"Use the following pieces of context to answer the question at the end. Do not provide commentary or elaboration more than 1 or 2 lines.?\"\n",
    "\n",
    "template = generate_prompt(\n",
    "    \"\"\"\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8952142d-ec91-4c67-9aa0-b9a71910fe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_992105/3144239959.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    streamer=streamer,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac8e41b5-cb60-4215-a297-400325190496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "```cpp\n",
      "#include <ros/ros.h>\n",
      "#include <std_msgs/String.h>\n",
      "\n",
      "class FibonacciActionServer : public rclcpp::Node {\n",
      "public:\n",
      "    explicit FibonacciActionServer(const rclcpp::NodeOptions & options = rclcpp::NodeOptions()) \n",
      "        : Node(\"bouncy_bolson\", options), loop_rate(10.0) {}\n",
      "\n",
      "    void handle_goal(const std_msgs::String::ConstPtr & msg) override {\n",
      "        // Handle goal message here.\n",
      "        std::cout << \"Goal received: \" << msg->data << std::endl;\n",
      "    }\n",
      "\n",
      "    void handle_cancel() override {\n",
      "        // Handle cancel request here.\n",
      "        std::cout << \"Cancel requested.\" << std::endl;\n",
      "    }\n",
      "\n",
      "    void handle_accept() override {\n",
      "        // Accept the goal and start execution.\n",
      "        std::cout << \"Goal accepted.\" << std::endl;\n",
      "    }\n",
      "};\n",
      "\n",
      "int main(int argc, char * argv[]) {\n",
      "    ros::init(argc, argv, \"bouncy_bolon\");\n",
      "    auto fibonacci_node = std::make_shared<FibonacciActionServer>();\n",
      "    ros::spin(fibonacci_node);\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "Note: The provided C++ code snippet does not implement the actual logic for the `Bouncy Bolson` task but rather demonstrates how an action server can be created in ROS 2 using C++. To complete this task, you would need to modify the `handle_goal()` method to perform some computation based on the input data (e.g., calculating the nth Fibonacci number).\n"
     ]
    }
   ],
   "source": [
    "ask = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "result = ask(\"Write a sample code for Bouncy Bolson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066187c-0111-4d9f-9d0d-5defa348cfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4654279-7e81-4a72-b1ce-c2dc8c501ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a8d90-ac6f-4e70-82c9-439df352c7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
